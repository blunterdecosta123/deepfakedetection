{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL-kAt3KcXTc",
        "outputId": "ee4aa775-1bc1-4402-ce62-f60f832a4626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-01-18 07:49:57--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  17.8MB/s    in 3.4s    \n",
            "\n",
            "2024-01-18 07:50:01 (17.8 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bunzip2 /content/shape_predictor_68_face_landmarks.dat.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQjR8CWMxlWZ",
        "outputId": "485fd008-c016-413b-d612-10ba3e0e95d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Visual_Speech_Recognition_for_Multiple_Languages'...\n",
            "remote: Enumerating objects: 277, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 277 (delta 33), reused 81 (delta 22), pack-reused 177\u001b[K\n",
            "Receiving objects: 100% (277/277), 69.77 MiB | 29.30 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n",
            "/content/Visual_Speech_Recognition_for_Multiple_Languages\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/\"\n",
        "!git clone https://github.com/mpc001/Visual_Speech_Recognition_for_Multiple_Languages.git\n",
        "%cd \"Visual_Speech_Recognition_for_Multiple_Languages\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqFh6CQ8x_1b",
        "outputId": "fdd9bed8-84e3-4a98-9d2c-fcaf4ee8dcfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.2)\n",
            "Collecting av\n",
            "  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-11.0.0\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.5.26)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.23.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.9 sounddevice-0.4.6\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install opencv-python\n",
        "!pip install scipy\n",
        "!pip install scikit-image\n",
        "!pip install av\n",
        "!pip install six\n",
        "!pip install mediapipe\n",
        "!pip install ffmpeg-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya-FpAp2yIEz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from pipelines.model import AVSR\n",
        "from pipelines.data.data_module import AVSRDataLoader\n",
        "from pipelines.detectors.mediapipe.detector import LandmarksDetector\n",
        "\n",
        "class InferencePipeline(torch.nn.Module):\n",
        "    def __init__(self, modality, model_path, model_conf, detector=\"mediapipe\", face_track=False, device=\"cuda:0\"):\n",
        "        super(InferencePipeline, self).__init__()\n",
        "        self.device = device\n",
        "        # modality configuration\n",
        "        self.modality = modality\n",
        "        self.dataloader = AVSRDataLoader(modality, detector=detector)\n",
        "        self.model = AVSR(modality, model_path, model_conf, rnnlm=None, rnnlm_conf=None, penalty=0.0, ctc_weight=0.1, lm_weight=0.0, beam_size=40, device=device)\n",
        "        if face_track and self.modality in [\"video\", \"audiovisual\"]:\n",
        "            self.landmarks_detector = LandmarksDetector()\n",
        "        else:\n",
        "            self.landmarks_detector = None\n",
        "\n",
        "\n",
        "    def process_landmarks(self, data_filename, landmarks_filename):\n",
        "        if self.modality == \"audio\":\n",
        "            return None\n",
        "        if self.modality in [\"video\", \"audiovisual\"]:\n",
        "            landmarks = self.landmarks_detector(data_filename)\n",
        "            return landmarks\n",
        "\n",
        "\n",
        "    def forward(self, data_filename, landmarks_filename=None):\n",
        "        assert os.path.isfile(data_filename), f\"data_filename: {data_filename} does not exist.\"\n",
        "        landmarks = self.process_landmarks(data_filename, landmarks_filename)\n",
        "        data = self.dataloader.load_data(data_filename, landmarks)\n",
        "        transcript = self.model.infer(data)\n",
        "        return transcript\n",
        "\n",
        "    def extract_features(self, data_filename, landmarks_filename=None, extract_resnet_feats=False):\n",
        "        assert os.path.isfile(data_filename), f\"data_filename: {data_filename} does not exist.\"\n",
        "        landmarks = self.process_landmarks(data_filename, landmarks_filename)\n",
        "        data = self.dataloader.load_data(data_filename, landmarks)\n",
        "        with torch.no_grad():\n",
        "            if isinstance(data, tuple):\n",
        "                enc_feats = self.model.model.encode(data[0].to(self.device), data[1].to(self.device), extract_resnet_feats)\n",
        "            else:\n",
        "                enc_feats = self.model.model.encode(data.to(self.device), extract_resnet_feats)\n",
        "        return enc_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1HeUrFiyUnz",
        "outputId": "fe75a92b-0815-4cb0-c7bc-0c7e2f2ac534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-01-18 07:51:21--  http://www.doc.ic.ac.uk/~pm4115/autoAVSR/LRS3_V_WER19.1.zip\n",
            "Resolving www.doc.ic.ac.uk (www.doc.ic.ac.uk)... 146.169.13.6\n",
            "Connecting to www.doc.ic.ac.uk (www.doc.ic.ac.uk)|146.169.13.6|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 937274463 (894M) [application/zip]\n",
            "Saving to: ‘/content/data/LRS3_V_WER19.1.zip’\n",
            "\n",
            "/content/data/LRS3_ 100%[===================>] 893.85M  21.4MB/s    in 44s     \n",
            "\n",
            "2024-01-18 07:52:06 (20.3 MB/s) - ‘/content/data/LRS3_V_WER19.1.zip’ saved [937274463/937274463]\n",
            "\n",
            "Archive:  /content/data/LRS3_V_WER19.1.zip\n",
            "  inflating: /content/data/LRS3_V_WER19.1/model.json  \n",
            "  inflating: /content/data/LRS3_V_WER19.1/model.pth  \n"
          ]
        }
      ],
      "source": [
        "%mkdir -p /content/data/\n",
        "!wget http://www.doc.ic.ac.uk/~pm4115/autoAVSR/LRS3_V_WER19.1.zip -O /content/data/LRS3_V_WER19.1.zip\n",
        "!unzip -o /content/data/LRS3_V_WER19.1.zip -d /content/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj8T9tffJKzd",
        "outputId": "a035c048-3e5f-4a75-9b9c-2b012b2c5773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-stf1j1iv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-stf1j1iv\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=faca60a99399b7b295f5355b7a492de09f8c44a1a1d736faadd8602e6e37a2f3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zk93son9/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.2\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.23.5)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install moviepy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPzPXDT4JF5f"
      },
      "source": [
        "**Audio To Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2XBC6dyJIBt"
      },
      "outputs": [],
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "import whisper\n",
        "import os\n",
        "import logging\n",
        "\n",
        "\n",
        "class ExtractAudio:\n",
        "    def __init__(self, video_path, output_audio_path):\n",
        "        \"\"\"\n",
        "        Initializes an instance of the ExtractAudio class.\n",
        "\n",
        "        Args:\n",
        "            video_path (str): The path to the video file.\n",
        "            output_audio_path (str): The path to save the extracted audio file.\n",
        "        \"\"\"\n",
        "        self.video_path = video_path\n",
        "        self.output_audio_path = output_audio_path\n",
        "        self.model = whisper.load_model(\"base\")\n",
        "\n",
        "    def extract_and_transcribe_segment(self, start_time, end_time):\n",
        "        \"\"\"\n",
        "        Extracts audio from a specific segment of the video, transcribes it, and returns the transcribed text.\n",
        "\n",
        "        Args:\n",
        "            start_time (float): The start time of the segment in seconds.\n",
        "            end_time (float): The end time of the segment in seconds.\n",
        "\n",
        "        Returns:\n",
        "            str: The transcribed text of the audio segment.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with VideoFileClip(self.video_path) as video:\n",
        "                # Extract only the specified segment\n",
        "                audio_segment = video.subclip(start_time, end_time)\n",
        "                audio_segment.audio.write_audiofile(\n",
        "                    self.output_audio_path, codec=\"libmp3lame\"\n",
        "                )\n",
        "\n",
        "                # Transcribe the extracted audio\n",
        "                result = self.model.transcribe(self.output_audio_path)\n",
        "                return result[\"text\"]\n",
        "        except FileNotFoundError:\n",
        "            logging.error(\"Audio file not found.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in audio extraction and transcription: {e}\")\n",
        "        finally:\n",
        "            if os.path.exists(self.output_audio_path):\n",
        "                os.remove(self.output_audio_path)\n",
        "\n",
        "    def extract_and_transcribe_segments(self, segments):\n",
        "        \"\"\"\n",
        "        Extracts and transcribes audio for multiple segments.\n",
        "\n",
        "        Args:\n",
        "            segments (list): A list of tuples representing the start and end times of each segment.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of tuples containing the transcribed text, start time, and end time for each segment.\n",
        "        \"\"\"\n",
        "        transcriptions = []\n",
        "        for start_time, end_time in segments:\n",
        "            transcription = self.extract_and_transcribe_segment(start_time, end_time)\n",
        "            transcriptions.append((transcription, start_time, end_time))\n",
        "        return transcriptions\n",
        "\n",
        "    def extract_full_audio(self):\n",
        "        \"\"\"\n",
        "        Extracts the full audio from the video.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with VideoFileClip(self.video_path) as video:\n",
        "                video.audio.write_audiofile(self.output_audio_path, codec=\"libmp3lame\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"Video file not found.\")\n",
        "\n",
        "    def transcribe_full_audio(self):\n",
        "        \"\"\"\n",
        "        Transcribes the full audio of the video.\n",
        "\n",
        "        Returns:\n",
        "            str: The transcribed text of the full audio.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return self.model.transcribe(self.output_audio_path)[\"text\"]\n",
        "        finally:\n",
        "            if os.path.exists(self.output_audio_path):\n",
        "                os.remove(self.output_audio_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjixfWS3JiBM"
      },
      "source": [
        "**Similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii7G3p19JlLr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class BertSimilarity:\n",
        "    \"\"\"\n",
        "    Class for calculating cosine similarity between sentences using BERT embeddings,\n",
        "    optimized for GPU acceleration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Check if CUDA is available and set the device to GPU or CPU\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Load the model and tokenizer\n",
        "        self.model = BertModel.from_pretrained(\"bert-base-uncased\").to(self.device)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    def extract_sentences(self, data):\n",
        "        \"\"\"\n",
        "        Extracts sentences from the given data which is a list of tuples.\n",
        "        Each tuple contains a sentence and its corresponding timestamps.\n",
        "\n",
        "        :param data: List of tuples, where each tuple is (sentence, start_time, end_time)\n",
        "        :return: List of sentences.\n",
        "        \"\"\"\n",
        "        return [sentence[0] for sentence in data]\n",
        "\n",
        "    def calculate_similarity(self, sentences1, sentences2):\n",
        "        \"\"\"\n",
        "        Calculates the cosine similarity between the sentences of two datasets.\n",
        "\n",
        "        :param sentences1: First set of sentences.\n",
        "        :param sentences2: Second set of sentences.\n",
        "        :return: Matrix of cosine similarity scores.\n",
        "        \"\"\"\n",
        "        # Tokenize and prepare the sentences for the BERT model\n",
        "        inputs1 = self.tokenizer(\n",
        "            sentences1, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        inputs2 = self.tokenizer(\n",
        "            sentences2, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Process sentences through the model\n",
        "        with torch.no_grad():\n",
        "            outputs1 = self.model(**inputs1)\n",
        "            outputs2 = self.model(**inputs2)\n",
        "\n",
        "        # Use the [CLS] token embeddings for similarity\n",
        "        cls_embeddings1 = outputs1[0][:, 0, :]\n",
        "        cls_embeddings2 = outputs2[0][:, 0, :]\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarity = cosine_similarity(\n",
        "            cls_embeddings1.detach().cpu().numpy(),\n",
        "            cls_embeddings2.detach().cpu().numpy(),\n",
        "        )\n",
        "\n",
        "        return similarity\n",
        "\n",
        "    def plot_similarity_and_accuracy(self, data1, data2):\n",
        "        \"\"\"\n",
        "        Plots the similarity between two sets of sentences and shows the average accuracy.\n",
        "\n",
        "        :param data1: The first set of sentences (as a list of tuples).\n",
        "        :param data2: The second set of sentences (as a list of tuples).\n",
        "        :return: Cosine similarity score.\n",
        "        \"\"\"\n",
        "        sentences1 = self.extract_sentences(data1)\n",
        "        sentences2 = self.extract_sentences(data2)\n",
        "\n",
        "        sentences1 = ' '.join(sentences1)\n",
        "        sentences2 = ' '.join(sentences2)\n",
        "        similarities = self.calculate_similarity([sentences1], [sentences2])[0][0]\n",
        "\n",
        "        return similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGxe-bJRJtGi"
      },
      "source": [
        "<div style = \"background-color: Red\">Lips Detection: Give the ranges in which lips got detected</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "4x8TTTpPhaqD",
        "outputId": "10bc77ec-62d4-44ed-a3f0-72378733de0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: opencv-python 4.8.0.76\n",
            "Uninstalling opencv-python-4.8.0.76:\n",
            "  Successfully uninstalled opencv-python-4.8.0.76\n",
            "Found existing installation: opencv-python-headless 4.9.0.80\n",
            "Uninstalling opencv-python-headless-4.9.0.80:\n",
            "  Successfully uninstalled opencv-python-headless-4.9.0.80\n",
            "Collecting opencv-python-headless==4.5.5.64\n",
            "  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless==4.5.5.64) (1.23.5)\n",
            "Installing collected packages: opencv-python-headless\n",
            "Successfully installed opencv-python-headless-4.5.5.64\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip uninstall -y opencv-python opencv-python-headless\n",
        "!pip install opencv-python-headless==4.5.5.64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-FbAzpfJup5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "\n",
        "class LipDetector:\n",
        "\n",
        "    def __init__(self, video_path, merge_gap_ms=150):\n",
        "        self.video_path = video_path\n",
        "        self.detector = dlib.get_frontal_face_detector()\n",
        "        self.predictor = dlib.shape_predictor(\n",
        "            \"/content/shape_predictor_68_face_landmarks.dat\"\n",
        "        )\n",
        "        self.merge_gap_ms = merge_gap_ms\n",
        "\n",
        "    def process_video(self):\n",
        "        video = cv2.VideoCapture(self.video_path)\n",
        "        fps = video.get(cv2.CAP_PROP_FPS)\n",
        "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        current_frame = 0\n",
        "        lips_detected_segments = []\n",
        "        segment_start = None\n",
        "\n",
        "        while current_frame < total_frames:\n",
        "            video.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
        "            ret, frame = video.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            faces = self.detector(gray)\n",
        "            lips_detected = False\n",
        "\n",
        "            for face in faces:\n",
        "                landmarks = self.predictor(gray, face)\n",
        "                if any(landmarks.part(n) for n in range(48, 68)):\n",
        "                    lips_detected = True\n",
        "                    break\n",
        "\n",
        "            if lips_detected:\n",
        "                if segment_start is None:\n",
        "                    segment_start = current_frame / fps\n",
        "            elif segment_start is not None:\n",
        "                lips_detected_segments.append((segment_start, current_frame / fps))\n",
        "                segment_start = None\n",
        "\n",
        "            current_frame += 1\n",
        "\n",
        "        video.release()\n",
        "\n",
        "        # Add the last segment if it ends with lip detection\n",
        "        if segment_start is not None:\n",
        "            lips_detected_segments.append((segment_start, total_frames / fps))\n",
        "\n",
        "        return lips_detected_segments\n",
        "\n",
        "    def merge_segments(self, lips_detected_segments):\n",
        "        merged_segments = []\n",
        "        for start, end in lips_detected_segments:\n",
        "            if (\n",
        "                merged_segments\n",
        "                and start - merged_segments[-1][1] <= self.merge_gap_ms / 1000.0\n",
        "            ):\n",
        "                merged_segments[-1] = (merged_segments[-1][0], end)\n",
        "            else:\n",
        "                merged_segments.append((start, end))\n",
        "\n",
        "        return merged_segments\n",
        "\n",
        "    def detect_lips(self):\n",
        "        lips_detected_segments = self.process_video()\n",
        "        return self.merge_segments(lips_detected_segments)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5v5yGq9J1MJ"
      },
      "source": [
        "Transcribe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5D3gq3FJ2_J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "\n",
        "\n",
        "class VideoTranscriber:\n",
        "\n",
        "    def __init__(self, video_path, model_conf, model_path, device=\"cuda:0\"):\n",
        "        self.video_path = video_path\n",
        "        self.model_conf = model_conf\n",
        "        self.model_path = model_path\n",
        "        self.device = device\n",
        "        self.pipeline = InferencePipeline(\n",
        "            \"video\",\n",
        "            self.model_path,\n",
        "            self.model_conf,\n",
        "            face_track=True,\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "    def transcribe(self, lip_detected_segments, chunk_length=3, min_chunk_length=1):\n",
        "\n",
        "        transcripts = []\n",
        "\n",
        "        for segment in lip_detected_segments:\n",
        "            start_time, end_time = segment\n",
        "            segment_duration = end_time - start_time\n",
        "\n",
        "            # Check if the segment is shorter than the minimum chunk length\n",
        "            if segment_duration < min_chunk_length:\n",
        "                if transcripts:\n",
        "                    # Extend the previous chunk if there is one\n",
        "                    transcripts[-1] = (transcripts[-1][0], start_time, end_time)\n",
        "                continue\n",
        "\n",
        "            current_time = start_time\n",
        "            while current_time < end_time:\n",
        "                chunk_end_time = min(current_time + chunk_length, end_time)\n",
        "                chunk_filename = f\"chunk_{current_time}_{chunk_end_time}.mp4\"\n",
        "                ffmpeg_extract_subclip(\n",
        "                    self.video_path,\n",
        "                    current_time,\n",
        "                    chunk_end_time,\n",
        "                    targetname=chunk_filename,\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    transcript = self.pipeline(chunk_filename)\n",
        "                    transcripts.append((transcript, current_time, chunk_end_time))\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing chunk {chunk_filename}: {e}\")\n",
        "                finally:\n",
        "                    if os.path.exists(chunk_filename):\n",
        "                        os.remove(chunk_filename)\n",
        "\n",
        "                current_time += chunk_length\n",
        "\n",
        "        return transcripts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1FrByNtaq1j",
        "outputId": "da93b334-0a65-4035-c2a0-dabab7e0415c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.31.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.3)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Collecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyngrok==4.1.1) (0.18.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok==4.1.1) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15963 sha256=79004f09948816bd2e9a4faff6b6b832f70bdde57c96fe307a79431af4f15a8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/7c/4c/632fba2ea8e88d8890102eb07bc922e1ca8fa14db5902c91a8\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install pyngrok==4.1.1\n",
        "!ngrok authtoken 2FO2IsRIOoPTMJdkAofEdRoZtnb_7DJJ7H9MfWmG9QP8hx8SW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdJ9dmH-4csn",
        "outputId": "0c45561c-04d7-4b1f-fe1d-2de2aeeae926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://d57c-34-125-190-8.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from werkzeug.utils import secure_filename\n",
        "import os\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "UPLOAD_FOLDER = '/content/uploaded_videos'\n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
        "\n",
        "model_conf = \"/content/data/LRS3_V_WER19.1/model.json\"\n",
        "model_path = \"/content/data/LRS3_V_WER19.1/model.pth\"\n",
        "\n",
        "# Ensure the upload folder exists\n",
        "os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
        "\n",
        "global video_path\n",
        "video_path = None\n",
        "\n",
        "@app.route('/lipDetection', methods=['GET'])\n",
        "def lipsdetector():\n",
        "    '''\n",
        "    Detects lips in the video and returns the merged lip detection segments.\n",
        "    '''\n",
        "\n",
        "    global video_path  # Use the global keyword to access the global variable\n",
        "\n",
        "    lip_detector = LipDetector(video_path)\n",
        "    lips_detected_segments = lip_detector.detect_lips()\n",
        "\n",
        "    # Returning the data with a specific key\n",
        "    return jsonify({\"lips_detected_segments\": lips_detected_segments})\n",
        "\n",
        "@app.route('/videoTranscribe', methods=['GET', 'POST'])\n",
        "def videoTranscribe():\n",
        "    \"\"\"\n",
        "    Transcribes video segments based on lips_detected_segments received from the request.\n",
        "    \"\"\"\n",
        "    global video_path  # Use the global keyword to access the global variable\n",
        "    data = request.get_json()\n",
        "    lips_detected_segments = data.get('lips_detected_segments')\n",
        "    if not lips_detected_segments:\n",
        "        return jsonify({\"error\": \"No lips_detected_segments provided\"}), 400\n",
        "\n",
        "    # Assuming you pass the lips_detected_segments directly to the VideoTranscriber\n",
        "    transcriber = VideoTranscriber(video_path, model_conf, model_path)\n",
        "    transcripted_text = transcriber.transcribe(\n",
        "        lips_detected_segments, chunk_length=3, min_chunk_length=1\n",
        "    )\n",
        "    return jsonify({\"transcripted_text\": transcripted_text})\n",
        "\n",
        "@app.route('/audioTranscribe', methods=['GET', 'POST'])\n",
        "def audioTranscribe():\n",
        "    \"\"\"\n",
        "    Extracts and transcribes audio from video segments.\n",
        "    \"\"\"\n",
        "    global video_path  # Use the global keyword to access the global variable\n",
        "    # video_path = request.args.get('video_path')\n",
        "    data = request.get_json()\n",
        "    lips_detected_segments = data.get('lips_detected_segments')\n",
        "    if not lips_detected_segments:\n",
        "        return jsonify({\"error\": \"No lips_detected_segments pro\"})\n",
        "\n",
        "    audio_transcriber = ExtractAudio(video_path, \"temp_audio.wav\")\n",
        "    extracted_text = audio_transcriber.extract_and_transcribe_segments(\n",
        "        lips_detected_segments\n",
        "    )\n",
        "    return jsonify({\"extracted_text\": extracted_text})\n",
        "\n",
        "@app.route('/bertSimilarity', methods=['GET', 'POST'])\n",
        "def similarity():\n",
        "    data = request.get_json()\n",
        "\n",
        "    transcripted_text = data.get('transcripted_text')\n",
        "    extracted_text = data.get('extracted_text')\n",
        "\n",
        "    bert_similarity = BertSimilarity()\n",
        "\n",
        "    similarity_scores = bert_similarity.plot_similarity_and_accuracy(\n",
        "        transcripted_text, extracted_text\n",
        "    )\n",
        "    similarity_score = str(similarity_scores)\n",
        "    return jsonify({\"score\": similarity_score})\n",
        "\n",
        "@app.route('/upload', methods=['GET', 'POST'])\n",
        "def video_upload():\n",
        "\n",
        "    global video_path  # Reference the global variable\n",
        "\n",
        "    if 'video' not in request.files:\n",
        "        return jsonify({\"error\": \"No video file provided\"})\n",
        "\n",
        "    uploaded_file = request.files['video']\n",
        "\n",
        "    if uploaded_file.filename == '':\n",
        "        return jsonify({\"error\": \"No selected file\"})\n",
        "\n",
        "    if uploaded_file:\n",
        "        filename = secure_filename(uploaded_file.filename)\n",
        "        video_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
        "        uploaded_file.save(video_path)\n",
        "        return jsonify({\"message\": \"Video uploaded successfully\", \"path\": video_path})\n",
        "\n",
        "    return jsonify({\"error\": \"File upload failed\"})\n",
        "\n",
        "@app.route('/delete', methods=['GET', 'POST'])\n",
        "def deleteVideo():\n",
        "    # video_path = request.args.get('video_path') # or request.form['video_path']\n",
        "    if os.path.exists(video_path):\n",
        "        os.remove(video_path)\n",
        "        return jsonify({\"message\": \"Video deleted successfully\"})\n",
        "    else:\n",
        "        return jsonify({\"error\": \"Video not found\"})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
